{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import polars as pl\n",
    "\n",
    "from ethos.constants import PROJECT_ROOT\n",
    "from ethos.inference.constants import Task\n",
    "\n",
    "# results from Google Drive\n",
    "result_dir = PROJECT_ROOT / \"results\"\n",
    "# data dir with tokenized mimic_ed\n",
    "data_dir = PROJECT_ROOT / \"data\"\n",
    "tokenized_data_dir = data_dir / \"tokenized_datasets\" / \"mimic_ed_procedures/test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ethos.datasets import (\n",
    "    CriticalOutcomeAtTriageDataset,\n",
    "    EdReattendenceDataset,\n",
    "    HospitalAdmissionAtTriageDataset,\n",
    "    TimelineDataset,\n",
    ")\n",
    "\n",
    "gts = {\n",
    "    Task.ED_HOSPITALIZATION: HospitalAdmissionAtTriageDataset,\n",
    "    Task.ED_CRITICAL_OUTCOME: CriticalOutcomeAtTriageDataset,\n",
    "    Task.ED_REPRESENTATION: EdReattendenceDataset,\n",
    "}\n",
    "\n",
    "\n",
    "def dataset2df(dataset):\n",
    "    return pl.from_dicts(y for _, y in dataset).with_columns(\n",
    "        pl.col(\"prediction_time\").cast(pl.Datetime)\n",
    "    )\n",
    "\n",
    "\n",
    "gts = {task: dataset2df(cls(tokenized_data_dir)) for task, cls in gts.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the task to see the label mismatch\n",
    "task = Task.ED_REPRESENTATION\n",
    "\n",
    "ed_bench_dir = result_dir / \"baseline_ed_bench\"\n",
    "\n",
    "ed_bench_results = (\n",
    "    pl.scan_parquet((ed_bench_dir / task).with_suffix(\".parquet\"))\n",
    "    .with_columns(\n",
    "        pl.col(\"intime\", \"outtime\").str.strptime(pl.Datetime, \"%Y-%m-%d %H:%M:%S\"),\n",
    "    )\n",
    "    .sort(\"subject_id\", \"intime\", \"outtime\")\n",
    "    .collect()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This can be generated in `notebooks/all_task_label_dumps.ipynb`\n",
    "ed_task_labels_dir = data_dir / \"ed_task_labels\"\n",
    "\n",
    "ethos_labels = (\n",
    "    pl.scan_parquet((ed_task_labels_dir / task).with_suffix(\".parquet\"))\n",
    "    .filter(fold=\"test\")\n",
    "    .sort(\"subject_id\", \"time\")\n",
    "    .collect()\n",
    ")\n",
    "mismatch = ethos_labels.join(\n",
    "    ed_bench_results,\n",
    "    left_on=[\"subject_id\", \"time\"],\n",
    "    right_on=[\"subject_id\", \"outtime\" if task == Task.ED_REPRESENTATION else \"intime\"],\n",
    "    how=\"full\",\n",
    ").filter(pl.col(\"boolean_value\") != pl.col(\"boolean_value_right\"))\n",
    "mismatch.join(\n",
    "    gts[task],\n",
    "    left_on=[\"subject_id\", \"time\"],\n",
    "    right_on=[\"patient_id\", \"prediction_time\"],\n",
    "    how=\"left\",\n",
    "    maintain_order=\"left\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print timeline fragments for the selected label mismatch\n",
    "n = 0\n",
    "\n",
    "d = TimelineDataset(tokenized_data_dir)\n",
    "\n",
    "data_idx = mismatch.join(\n",
    "    gts[task],\n",
    "    left_on=[\"subject_id\", \"time\"],\n",
    "    right_on=[\"patient_id\", \"prediction_time\"],\n",
    "    maintain_order=\"left\",\n",
    ").row(n, named=True)[\"data_idx\"]\n",
    "\n",
    "timeline_start = data_idx\n",
    "timeline_end = d.patient_data_end_at_idx[timeline_start].item() + 5\n",
    "print(timeline_start, timeline_end)\n",
    "\n",
    "timeline_slice = slice(timeline_start, timeline_end)\n",
    "tokens = d.vocab.decode(d.tokens[timeline_slice])\n",
    "times = d.times[timeline_slice].tolist()\n",
    "timeline = pl.DataFrame(\n",
    "    [tokens, times], schema={\"token\": pl.Utf8, \"time\": pl.Datetime}\n",
    ").with_row_index(offset=timeline_start)\n",
    "timeline"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
